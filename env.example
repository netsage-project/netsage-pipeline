# Sensor name to assign
# == EXAMPLE VALUES MUST BE REPLACED == 
sflowSensorName=The Sflow Sensor Name
netflowSensorName=The Netflow Sensor Name

# Final Logstash output is to a rabbit queue,
# the default is to write to the local rabbitmq server.
# === FOR SENDING PROCESSED FLOWS TO GlobalNOC, ASK FOR THE PROPER SETTINGS ===
rabbitmq_output_host=rabbit
rabbitmq_output_username=guest
rabbitmq_output_pw=guest
rabbitmq_output_key=netsage_archive_input

# To filter flows by sensor(s) and interface(s)
# (see the Docker Advanced documentation)
# "ALL" can refer to all sensors or all interfaces of a sensor.
# If a sensor is not references, all flows will be kept.
ifindex_filter_flag=False
#example: ifindex_filter_keep= Sensor-1: 456,789; Sensor 2: ALL

# To change the sensor name for flows coming from a given sensor and using a certain interface.
# (See the Docker Advanced documentation)
ifindex_sensor_rename_flag=False
#example: ifindex_sensor_rename_ifindex=123
#example: ifindex_sensor_rename_old_name=old name
#example: ifindex_sensor_rename_new_name=new name

# To manually correct flow sizes and rates for sampling, specified sensor(s) only.
# (See the Docker Advanced documentation. This is uncommon.)
sampling_correction_flag=False
#example: sampling_correction_sensors=sensor1,sensor2
#example: sampling_correction_factor=100

# Logstash Aggregation Filter settings
# default inactivity_timeout is 5-minute 
# default max_flow_timeout is 1 hour. This is the maximum allowed flow duration; longer flows will be broken up.
inactivity_timeout=300
max_flow_timeout=3600
aggregation_maps_path=/data/logstash-aggregation-maps

# Logstash settings
# set this to false so we don't install elasticsearch locally 
XPACK_MONITORING_ENABLED=false
# java heap size for logstash
LS_JAVA_OPTS=-Xmx2g -Xms2g
# the logstash aggregation filter requires that only one logstash worker is running
PIPELINE_WORKERS=1
# for debugging
## LOG_LEVEL=debug

# Local RabbitMQ Server config
RABBITMQ_ERLANG_COOKIE='secret cookie'
RABBIT_HOST=rabbit
RABBITMQ_DEFAULT_USER=guest
RABBITMQ_DEFAULT_PASS=guest
discovery.type=single-node

# Importer output rabbit host = Logstash input rabbit host
# default is to use the local rabbitmq server
rabbitmq_input_host=rabbit
rabbitmq_input_username=guest
rabbitmq_input_pw=guest

# In case you run elasticsearch and kibana
ELASTIC_HOSTNAME='elastic'
