# This filter stitches together incoming flows that go together.
#
## Fields most likely to be specific to a logstash pipeline:
## You may set these via environment variables 
##    aggregate_maps_path  - must be unique for each logstash pipeline. Default is /tmp/logstash-aggregation-maps.
##    inactivity_timeout - default is 5 min.
##    timeout - the maximum length of a flow.  Default is 1 hr.
## NOTE THAT THERE ARE SEPARATE SECTIONS FOR SFLOW AND NETFLOW,
## EDIT BOTH !!!!

filter {

    # === TSTAT ===
    # Tstat only reports complete flows, so no stitching is needed!
    # Just add stitched_flows=0 (means no stitching attempted)
    if [meta][flow_type] == 'tstat' {
      mutate {
        id => "40-1"
        add_field => { 'stitched_flows' => 0 }
      }
    }
    else {
    # for aggregation, we need the 'start' or 'end' date, as well as as timestamp
      date {
          id => "40-2"
          match  => [ '[start]', 'UNIX' ]
          target => '[start_date]'
      }
      date {
          id => "40-3"
          match  => [ '[end]', 'UNIX' ]
          target => '[end_date]'
      }
    }

    # === SFLOW ===
    # Aggregate on hash of 5-tuple + sensor
    # Incoming events may be single samples or results from partial aggregation/stitching by sfacctd.
    if [meta][flow_type] == "sflow" {
      aggregate {
          id => "40-4"
          # Events that have matching task_id's will be aggregated.
          task_id => '%{[flow_fingerprint]}'

          # Save the task_id value to this field in the aggregated event on timeout
          timeout_task_id_field => "[flow_fingerprint]"

          # Use this field when determining if timeouts have occurred, in case we are processing historical data.
          # It'll actually look at values of this field AND the clock times at which events come in. (Must be type 'date')
          timeout_timestamp_field => '[start_date]'

          # Inactive timeout
          # A flow is assumed to have ended if more than inactivity_timeout seconds have passed since the last matching event.
          # (Aggregator compares timeout_timestamp_field of the current matching event and of the last matching event. If the diff is 
          # greater than inactivity_timeout, it ends the current flow and starts a new one.
          # ALSO, every 5 sec, it compares the ingest clock time of the last matching event to NOW.
          # If more than inactivity_timeout seconds have passed, it declares the flow finished.)
          ##  default 300 sec = 5 min
          inactivity_timeout => "${inactivity_timeout:300}"

          # Active timeout
          # = maximum possible flow duration
          # (Aggregator compares timeout_timestamp_field of the current event to that of the FIRST event in the map. If the
          # diff is greater than timeout, it ends the current flow and starts a new one, even if matching events are still coming in.
          # ALSO, every 5 sec, it compares the ingest clock time of the first event in the map to NOW.
          # If more than timeout seconds have passed, it declares the flow finished, even if matching events are still coming in.)
          ##  default 3600 sec = 1 hour
          timeout => "${max_flow_timeout:3600}"

          # Save the aggregation map as a new event upon timeout
          push_map_as_event_on_timeout => true

          # Save all the in-progress aggregation maps when logstash shuts down, to be read back in when it restarts.
          ##  (use a different file for each logstash pipeline!)
          aggregate_maps_path => '${aggregation_maps_path:/tmp/logstash-aggregation-maps}'

          # Ruby code to run for each event.
          # (The event will be added to the correct map (hash) according to its task_id.
          # ||= assigns the value only if the variable does not yet exist. Only map values are included in the final event.)
          code => "
            # keep track of how many events we aggregate
            map['stitched_flows'] ||= 0
            map['stitched_flows'] += 1

            # map[start and end] are start and end times of the full stitched flow (timestamps)
            map['start'] ||= event.get('start')
            map['end']   ||= event.get('end')

            # Save these fields from the FIRST event.
            # Only 'values' will be updated as we stitch events or at the very end.
            map['meta']   ||= event.get('meta')
            map['values'] ||= event.get('values')
            map['tags']   ||= event.get('tags')
            map['@sampling_corrected'] ||= event.get('@sampling_corrected')

            # Essentially the time the flow entered the pipeline
            map['@ingest_time'] ||= Time.now     # Saving @timestamp caused problems when aggregate map was saved to a file then read.
                                                 # but this works.
                                                 # An @timestamp will be added when the map is finally pushed as an event.

       #### FOR TESTING        (EDIT IN BOTH SFLOW AND NETFLOW SECTIONS !!!)
             #map['trial'] = 1
             #map['values']['indivDurations'] ||= ' '
             #map['values']['indivDurations'] += event.get('[values][duration]').to_f.round(3).to_s
             #map['values']['indivDurations'] += '; '
       ####

            # If we are seeing a subsequent flow event... (assumes all events are in order!)
            if map['stitched_flows'] > 1
                map['end']  =  event.get('end')
                # sum the packet and bit counters
                map['values']['num_packets'] += event.get('[values][num_packets]')
                map['values']['num_bits'] += event.get('[values][num_bits]')
            end

            # Discard the original events. We only care about the aggregation.
            event.cancel()
          "

          # Code to run on the new aggregated event before it's pushed out
          timeout_code => "
                # recalculate total duration
                duration = event.get('end') - event.get('start')
                event.set( '[values][duration]', duration.round(3) )

                # recalculate average pps and bps  (say duration < .001 is 0 within roundoff error)
                if duration >= 0.001
                    event.set( '[values][packets_per_second]', event.get('[values][num_packets]') / duration )
                    event.set( '[values][bits_per_second]', event.get('[values][num_bits]') / duration )
                else
                # can't calculate (accurate) rates so set to 0
                    event.set( '[values][duration]', 0.0 )
                    event.set( '[values][packets_per_second]', 0.0 )
                    event.set( '[values][bits_per_second]', 0.0 )
                end
          "
      }
    }
    # === NETFLOW ===
    # Aggregate on hash of 5-tuple + sensor + start time
    # We have to do special things due to the fact that netflow sensors send "updates" about active flows,
    # all with the same start time, but bytes and packets are not cumulative.
    # The following will aggregate the updates up to 1 hr; and it will adjust start times when long flows are split up into 1 hr chunks.
    # Note that when there's a timeout at the router (default inactive timeout is usually 15 sec), the flows will stay separate
    # and not be stitched, even though they have the same 5-tuple, since the start time will change.
    else if [meta][flow_type] == "netflow" {
      ruby {
          # if duration is > timeout (1 hr), adjust start time to cut off n*timeout (whole hours).
          # That part of the flow should have already been processed and pushed out.
          id => "40-5"
          tag_on_exception => '_rubyexception in 40-aggregation.conf'
          code => "
              start = event.get( 'start' )
              duration = event.get( '[values][duration]' ).to_f
              cuts = 0      # how many times the start time got cut
              while duration > 3600.0
                  start = start + 3600.0  # move start forward
                  duration -= 3600.0
                  cuts += 1
              end
              if cuts > 0
                  event.set( 'start', start )
                  event.set( '[values][duration]', duration )
                  event.set( '@dur_cuts', cuts )  #### FOR TESTING
              end
          "
      }
      aggregate {
          id => "40-6"
          # unique ID used to aggregate events   ## A second agg filter must have different task_id "pattern"
          # For Netflow, include start time so only "updates" with the same start time are aggregated, not
          # continuations after short gaps that the router considers timeouts.
          task_id => '%{[flow_fingerprint]}-%{[start]}'

          # see comments above. MAKE SURE THE VALUES/DEFAULTS ARE THE SAME HERE.
          timeout_timestamp_field => '[start_date]'
          inactivity_timeout => "${inactivity_timeout:300}"
          timeout => "${max_flow_timeout:3600}"
          push_map_as_event_on_timeout => true

          ## can only set this in 1 agg. filter and it is set above!
          ## aggregate_maps_path => '${aggregation_maps_path:/tmp/logstash-aggregation-maps}'

          # Ruby code to run for each event.
          code => "
            # we have to save flow_fingerprint explicitly for netflow
            map['flow_fingerprint'] ||= event.get('flow_fingerprint')

            map['stitched_flows'] ||= 0
            map['stitched_flows'] += 1
            map['start'] ||= event.get('start')
            map['end']   ||= event.get('end')
            map['meta']   ||= event.get('meta')
            map['values'] ||= event.get('values')
            map['tags']   ||= event.get('tags')
            map['@sampling_corrected'] ||= event.get('@sampling_corrected')
            map['@ingest_time'] ||= Time.now

       #### FOR TESTING        (EDIT IN BOTH SFLOW AND NETFLOW SECTIONS !!!)
             #map['trial'] = 1
             #map['values']['indivDurations'] ||= ' '
             #map['values']['indivDurations'] += event.get('[values][duration]').to_f.round(3).to_s
             #map['values']['indivDurations'] += '; '
       ####

            if map['stitched_flows'] > 1
                map['end']  =  event.get('end')
                map['values']['num_packets'] += event.get('[values][num_packets]')
                map['values']['num_bits'] += event.get('[values][num_bits]')
                map['@dur_cuts'] =  event.get('@dur_cuts')  #### FOR TESTING
            end

            event.cancel()
          "

          timeout_code => "
                duration = event.get('end') - event.get('start')
                event.set( '[values][duration]', duration.round(3) )

                if duration >= 0.001
                    event.set( '[values][packets_per_second]', event.get('[values][num_packets]') / duration )
                    event.set( '[values][bits_per_second]', event.get('[values][num_bits]') / duration )
                else
                    event.set( '[values][duration]', 0.0 )
                    event.set( '[values][packets_per_second]', 0.0 )
                    event.set( '[values][bits_per_second]', 0.0 )
                end
          "
        }
    } # end if netflow

}
