# This filter stitches together incoming flows that go together.

# Values for ${variable-name:default-value} are obtained from an environment file. For docker, from the .env file;
# for bare-metal installations, /etc/logstash/logstash-env-vars - specified in the logstash systemd file)
# If values are not provided in an env file, the defaults/examples following the :'s are used.
# With a bare-metal installation, you may also just edit this file and fill in the values you want.

## Fields most likely to be specific to a logstash pipeline:
##    aggregate_maps_path  - must be unique for each logstash pipeline. Default is /tmp/logstash-aggregation-maps.
##    inactivity_timeout - when to declare a flow ended.
##    timeout - the maximum length of a flow.  

## NOTE THAT THERE ARE SEPARATE SECTIONS FOR SFLOW AND NETFLOW,
## EDIT BOTH !!!!

filter {

    # === TSTAT ===
    # Tstat only reports complete flows, so no stitching is needed!
    # Just add stitched_flows=0 (means no stitching attempted)
    if [meta][flow_type] == 'tstat' {
        mutate {
          id => "40-1"
          add_field => { 'stitched_flows' => 0 }
        }
    }

    else {
    # for aggregation, we need the 'start' or 'end' date, as well as as timestamp
        date {
            id => "40-2"
            match  => [ '[start]', 'UNIX' ]
            target => '[start_date]'
        }
        date {
            id => "40-3"
            match  => [ '[end]', 'UNIX' ]
            target => '[end_date]'
        }
    }

    # === SFLOW ===
    # Aggregate on hash of 5-tuple + sensor
    # Incoming events may be single samples or results from partial aggregation/stitching by sfacctd.
    if [meta][flow_type] == "sflow" {
        aggregate {
            id => "40-4"
            # Events that have matching task_id's will be aggregated.
            task_id => '%{[flow_fingerprint]}'

            # Save the task_id value to this field in the aggregated event on timeout
            timeout_task_id_field => "[flow_fingerprint]"

            # Use this field when determining if timeouts have occurred, in case we are processing historical data.
            # It'll actually look at values of this field AND the clock times at which events come in. (Must be type 'date')
            timeout_timestamp_field => "[start_date]"

            # Inactive timeout
            # A flow is assumed to have ended if more than inactivity_timeout seconds have passed since the last matching event.
            # (Aggregator compares timeout_timestamp_field of the current matching event and of the last matching event. If the diff is 
            # greater than inactivity_timeout, it ends the current flow and starts a new one.
            # ALSO, every 5 sec, it compares the ingest clock time of the last matching event to NOW.
            # If more than inactivity_timeout seconds have passed, it declares the flow finished.)
            ##  default 360 sec = 6 min
            inactivity_timeout => "${inactivity_timeout:360}"

            # Active timeout
            # = maximum possible flow duration
            # (Aggregator compares timeout_timestamp_field of the current event to that of the FIRST event in the map. If the
            # diff is greater than timeout, it ends the current flow and starts a new one, even if matching events are still coming in.
            # ALSO, every 5 sec, it compares the ingest clock time of the first event in the map to NOW.
            # If more than timeout seconds have passed, it declares the flow finished, even if matching events are still coming in.)
            ##  default 3600 sec = 1 hour
            timeout => "${max_flow_timeout:3600}"

            # Save the aggregation map as a new event upon timeout
            push_map_as_event_on_timeout => true

            # Save all the in-progress aggregation maps to this file when logstash shuts down, to be read back in when it restarts.
            ##  (use a different file for each logstash pipeline!)
            aggregate_maps_path => '${aggregation_maps_path:/tmp/logstash-aggregation-maps}'

            # Ruby code to run for each event.
            # (The event will be added to the correct map (hash) according to its task_id.
            # ||= assigns the value only if the variable does not yet exist. Only map values are included in the final event.)
            code => "
                # keep track of how many events we aggregate
                map['stitched_flows'] ||= 0
                map['stitched_flows'] += 1
  
                # map[start and end] are start and end times of the full stitched flow (timestamps)
                map['start'] ||= event.get('start')
                map['end']   ||= event.get('end')

                # Save these fields from the FIRST event.
                # Only 'values' will be updated as we stitch events or at the very end !!!!!
                map['meta']   ||= event.get('meta')
                map['values'] ||= event.get('values')
                map['tags']   ||= event.get('tags') 
                map['@sampling_corrected'] ||= event.get('@sampling_corrected')

                # Essentially the time the flow entered the pipeline
                map['@ingest_time'] ||= Time.now     # Saving @timestamp caused problems when aggregate map was saved to a file then read.
                                                     # but this works.
                                                     # An @timestamp will be added when the map is finally pushed as an event.

         #### FOR TESTING        (EDIT IN BOTH SFLOW AND NETFLOW SECTIONS !!!)
               #map['trial'] = 1
               #map['values']['indivDurations'] ||= ' '
               #map['values']['indivDurations'] += event.get('[values][duration]').to_f.round(3).to_s
               #map['values']['indivDurations'] += '; '
         ####

                # If we are seeing a subsequent flow event... (assumes all events are in order!)
                if map['stitched_flows'] > 1
                    map['end']  =  event.get('end')
                    # sum the packet and bit counters
                    map['values']['num_packets'] += event.get('[values][num_packets]')
                    map['values']['num_bits'] += event.get('[values][num_bits]')
                end

                # Discard the original events. We only care about the aggregation.
                event.cancel()
            "

            # Code to run on the new aggregated event before it's pushed out
            timeout_code => "
                # recalculate total duration
                duration = event.get('end') - event.get('start')
                event.set( '[values][duration]', duration.round(3) )

                # recalculate average pps and bps  (say duration < .001 is 0 within roundoff error)
                if duration >= 0.001
                    event.set( '[values][packets_per_second]', event.get('[values][num_packets]') / duration )
                    event.set( '[values][bits_per_second]', event.get('[values][num_bits]') / duration )
                else
                # can't calculate (accurate) rates so set to 0
                    event.set( '[values][duration]', 0.0 )
                    event.set( '[values][packets_per_second]', 0.0 )
                    event.set( '[values][bits_per_second]', 0.0 )
                end
            "
        }
    }

    # === NETFLOW ===
    # Aggregate on hash of 5-tuple + sensor + start time
    # 
    # Before aggregating, we have to do special start-time adjustments due to the fact that netflow sensors send "updates" 
    # about active flows, all with the same start time, but bytes and packet counts are only for the time since the last update.
    # We will aggregate the updates up to max_flow_timeout (1 hr by default) then start a new aggregated flow. 
    # If a flow (update) comes in with a duration over max_flow_timeout, the start time will be adjusted. Multiples of 
    # max_flow_timeout (eg whole hours) will be cut off, since bits from those times should have already been accounted for in 
    # a previous aggregated flow.
    # Note that if there's a timeout at the router (default inactive timeout is usually only 15 sec), the incoming flows will stay 
    # separate and not be stitched here, even though they have the same 5-tuple, since the start time will change.
    else if [meta][flow_type] == "netflow" {
        mutate {
            add_field => { "[@metadata][max_dur]" => "${max_flow_timeout:3600}" }
            id => "40-5"
        }
        ruby {
            # if duration is > timeout (1 hr), adjust start time to cut off n*timeout (whole hours).
            # That part of the flow should have already been processed and pushed out.
            id => "40-6"
            tag_on_exception => "_rubyexception in 40-aggregation.conf"
            code => "
                max_dur = event.get( '[@metadata][max_dur]' ).to_f
                duration = event.get( '[values][duration]' ).to_f
                start = event.get( 'start' )
                cuts = 0      # how many times the start time got cut
                while duration > max_dur
                    start = start + max_dur  # move start forward
                    duration -= max_dur
                    cuts += 1
                end
                if cuts > 0
                    event.set( 'start', start )
                    event.set( '[values][duration]', duration )
                    event.set( '@dur_cuts', cuts )  #### no. of max_dur's cut off - FOR TESTING
                end
            "
        }
        aggregate {
            id => "40-7"
            # unique ID used to aggregate events   ## A second agg filter must have different task_id "pattern"
            # For Netflow, include start time so only "updates" with the same start time are aggregated, not
            # continuations after short gaps that the router considers timeouts.
            task_id => '%{[flow_fingerprint]}-%{[start]}'

            # see comments above. MAKE SURE THE VALUES/DEFAULTS ARE THE SAME HERE.
            timeout_timestamp_field => "[start_date]"
            inactivity_timeout => "${inactivity_timeout:360}"
            timeout => "${max_flow_timeout:3600}"
            push_map_as_event_on_timeout => true

            ## can only set this in 1 agg. filter and it is set above!
            ## aggregate_maps_path => '${aggregation_maps_path:/tmp/logstash-aggregation-maps}'

            # Ruby code to run for each event.
            code => "
                # we have to save flow_fingerprint explicitly for netflow
                map['flow_fingerprint'] ||= event.get('flow_fingerprint')

                map['stitched_flows'] ||= 0
                map['stitched_flows'] += 1
                map['start']  ||= event.get('start')
                map['end']    ||= event.get('end')
                map['meta']   ||= event.get('meta')
                map['values'] ||= event.get('values')
                map['tags']   ||= event.get('tags')    # saves first aggregated event only!
                map['@sampling_corrected'] ||= event.get('@sampling_corrected')    # saves first aggregated event only!
                map['@ingest_time'] ||= Time.now

         #### FOR TESTING        (EDIT IN BOTH SFLOW AND NETFLOW SECTIONS !!!)
               #map['trial'] = 1
               # For netflow updates, indiv durations will be the cumulative duration of the aggregated flow as it aggregates
               #map['values']['indivDurations'] ||= ' '
               #map['values']['indivDurations'] += event.get('[values][duration]').to_f.round(3).to_s
               #map['values']['indivDurations'] += '; '
         ####

               if map['stitched_flows'] > 1
                   map['end']  =  event.get('end')
                   map['values']['num_packets'] += event.get('[values][num_packets]')
                   map['values']['num_bits'] += event.get('[values][num_bits]')
                   map['@dur_cuts'] =  event.get('@dur_cuts')  #### FOR TESTING
               end

               event.cancel()
            "

            timeout_code => "
                duration = event.get('end') - event.get('start')
                event.set( '[values][duration]', duration.round(3) )

                if duration >= 0.001
                    event.set( '[values][packets_per_second]', event.get('[values][num_packets]') / duration )
                    event.set( '[values][bits_per_second]', event.get('[values][num_bits]') / duration )
                else
                    event.set( '[values][duration]', 0.0 )
                    event.set( '[values][packets_per_second]', 0.0 )
                    event.set( '[values][bits_per_second]', 0.0 )
                end
            "
        }
    } # end if netflow

}
