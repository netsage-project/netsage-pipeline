(window.webpackJsonp=window.webpackJsonp||[]).push([[124],{195:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return l})),n.d(t,"toc",(function(){return r})),n.d(t,"default",(function(){return u}));var a=n(3),s=n(7),i=(n(0),n(208)),o={id:"bare_metal_install",title:"Manual Installation Guide",sidebar_label:"Manual Installation"},l={unversionedId:"deploy/bare_metal_install",id:"deploy/bare_metal_install",isDocsHomePage:!1,title:"Manual Installation Guide",description:"This document covers installing and running the NetSage Flow Processing Pipeline manually (without using Docker). It assumes a RedHat Linux environment or one of its derivatives.",source:"@site/docs/deploy/bare_metal_install.md",slug:"/deploy/bare_metal_install",permalink:"/netsage-pipeline/docs/next/deploy/bare_metal_install",editUrl:"https://github.com/netsage-project/netsage-pipeline/edit/master/website/docs/deploy/bare_metal_install.md",version:"current",sidebar_label:"Manual Installation",sidebar:"Pipeline",previous:{title:"Choosing an Installation Procedure",permalink:"/netsage-pipeline/docs/next/deploy/choose_install"},next:{title:"Docker Installation Guide",permalink:"/netsage-pipeline/docs/next/deploy/docker_install_simple"}},r=[{value:"Data sources",id:"data-sources",children:[]},{value:"Installing the Prerequisites",id:"installing-the-prerequisites",children:[{value:"Installing Pmacct",id:"installing-pmacct",children:[]},{value:"Installing RabbitMQ",id:"installing-rabbitmq",children:[]},{value:"Installing Logstash",id:"installing-logstash",children:[]},{value:"Installing the Pipeline",id:"installing-the-pipeline",children:[]}]},{value:"Logstash Configuration Files",id:"logstash-configuration-files",children:[]},{value:"Pmacct Configuration and Unit Files",id:"pmacct-configuration-and-unit-files",children:[]},{value:"Start Logstash",id:"start-logstash",children:[]},{value:"Start Pmacct Processes",id:"start-pmacct-processes",children:[]},{value:"Cron jobs",id:"cron-jobs",children:[]}],c={toc:r};function u(e){var t=e.components,n=Object(s.a)(e,["components"]);return Object(i.b)("wrapper",Object(a.a)({},c,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"This document covers installing and running the NetSage Flow Processing Pipeline manually (without using Docker). It assumes a RedHat Linux environment or one of its derivatives."),Object(i.b)("h2",{id:"data-sources"},"Data sources"),Object(i.b)("p",null,"The Processing pipeline needs data to ingest in order to do anything. There are two types of data that can be consumed."),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},"sflow or netflow"),Object(i.b)("li",{parentName:"ol"},"tstat")),Object(i.b)("p",null,"At least one of these must be set up on a sensor to provide the incoming flow data."),Object(i.b)("p",null,"See the Docker Installation instuctions for more info."),Object(i.b)("h2",{id:"installing-the-prerequisites"},"Installing the Prerequisites"),Object(i.b)("h3",{id:"installing-pmacct"},"Installing Pmacct"),Object(i.b)("p",null,"The pmacct package provides nfacctd and sfacctd processes which receive flow data and write it to a rabbitmq queue."),Object(i.b)("p",null,"Since the pmacct devs have not released a tagged version (or docker containers) since 1.7.7, and we require some commits that fixed an issue for us on Oct 11, 2021, we need to build pmacct from master (or master from some time after Oct 11, 2021).  "),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"    1. Go to the host where you want to install or upgrade nfacctd and sfacctd\n    2. Get dependencies if they were not previously installed on the host (Netsage needs librabbitmq-devel and jansson-devel)\n         $ sudo yum install libpcap-devel  pkgconfig  libtool  autoconf  automake  make\n         $ sudo yum install libstdc++-devel  gcc-c++  librabbitmq-devel  jansson-devel.x86_64\n    3. Clone the repo\n         $ git clone https://github.com/pmacct/pmacct.git\n    4. Rename the dir to, eg, pmacct-02Jun2022/, using today's date or the date of the code you are going to check out. eg.\n         $ cd pmacct-02June2022\n    5. You should be in master at this point.\n       To build and install a specific release/tag/branch, just check out that tag/branch and proceed.   \n       We have done testing (and made docker images) with this version:\n         $ git checkout 865a81e1f6c444aab32110a87d72005145fd6f74\n    6. Get ready to build sfacctd and nfacctd   (the following options are needed for Netsage)\n         $ ./autogen.sh\n         $ ./configure --enable-rabbitmq --enable-jansson             \n    7. Build and install\n         $ make\n         $ sudo make install \n         $ make clean\n         $ make distclean\n    8. Check the versions\n         $ sfacctd -V\n         $ nfacctd -V\n       These should give something like this where 20220602 is the date:\n            nfacctd 1.7.8-git [20220602-0 (5e4b0612)]\n")),Object(i.b)("h3",{id:"installing-rabbitmq"},"Installing RabbitMQ"),Object(i.b)("p",null,"A local rabbitmq instance is used to hold flow data until logstash can retreive and process it. "),Object(i.b)("p",null,"Typically, the rabbitmq server runs on the same server as the pipeline itself, but if need be, you can separate them (for this reason, the Rabbit server is not automatically installed with the pipeline package)."),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-sh"},"[root@host ~]# yum install rabbitmq-server\n\n")),Object(i.b)("p",null,"Typically, the default configuration will work. Perform any desired Rabbit configuration, then, start RabbitMQ:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-sh"},"[root@host ~]# /sbin/service rabbitmq-server start \n          or # systemctl start rabbitmq-server.service\n")),Object(i.b)("p",null,"Being able to view the user interface in a browser window is very useful. Look up how to enable it."),Object(i.b)("h3",{id:"installing-logstash"},"Installing Logstash"),Object(i.b)("p",null,"See the logstash documentation. We are currently using Version 7.16.2."),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"Download and install the public signing key\n        sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\n\nCreate or edit /etc/yum.repos.d/ELK.repo\n        [logstash-7.x]\n        name=Elastic repository for 7.x packages\n        baseurl=https://artifacts.elastic.co/packages/7.x/yum\n        gpgcheck=1\n        gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\n        enabled=1\n        autorefresh=1\n        type=rpm-md\n\nInstall\n        sudo yum install logstash\n")),Object(i.b)("h3",{id:"installing-the-pipeline"},"Installing the Pipeline"),Object(i.b)("p",null,"Installing the Pipeline just copies config, cron, and systemd files to the correct locations. There are no longer any perl scripts to install."),Object(i.b)("p",null,"The last Pipeline package released by GlobalNOC (",Object(i.b)("strong",{parentName:"p"},"a non-pmacct version"),") is in the GlobalNOC Open Source Repo. You can use that, if the version you want is there, or you can just build the rpm from scratch, or manually copy files to the correct locations (the .spec file indicates where)."),Object(i.b)("p",null,"(At least formerly, some of our dependencies come from the EPEL repo. We probably don't need this repo anymore though.)"),Object(i.b)("p",null,"a. To use the GlobalNOC Public repo, for Red Hat/CentOS 7, create ",Object(i.b)("inlineCode",{parentName:"p"},"/etc/yum.repos.d/grnoc7.repo")," with the following content."),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"[grnoc7]\nname=GlobalNOC Public el7 Packages - $basearch\nbaseurl=https://repo-public.grnoc.iu.edu/repo/7/$basearch\nenabled=1\ngpgcheck=1\ngpgkey=https://repo-public.grnoc.iu.edu/repo/RPM-GPG-KEY-GRNOC7\n")),Object(i.b)("p",null,"The first time you install packages from the repo, you will have to accept the GlobalNOC repo key."),Object(i.b)("p",null,"Install the package using yum:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"[root@host ~]# yum install grnoc-netsage-pipeline\n")),Object(i.b)("p",null,"b. To build the rpm from a git checkout, "),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},'git clone https://github.com/netsage-project/netsage-pipeline.git\ngit checkout master  (or a branch)\ncd netsage-pipeline\nperl Makefile.PL\nmake rpm\nsudo yum install /<path>/rpmbuild/RPMS/noarch/grnoc-netsage-pipeline-2.0.0-1.el7.noarch.rpm\n     (use "reinstall" if the version number has not changed)\n')),Object(i.b)("p",null,"c. You could also just move files manually to where they need to go. It should be fairly obvious."),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"/etc/logstash/conf.d/"),Object(i.b)("li",{parentName:"ul"},"/etc/pmacct/"),Object(i.b)("li",{parentName:"ul"},"/etc/cron.d/"),Object(i.b)("li",{parentName:"ul"},"/usr/bin/"),Object(i.b)("li",{parentName:"ul"},"/etc/systemd/system/"),Object(i.b)("li",{parentName:"ul"},"/var/lib/grnoc/netsage/ and /etc/logstash/conf.d/support/  (cron downloads)")),Object(i.b)("h2",{id:"logstash-configuration-files"},"Logstash Configuration Files"),Object(i.b)("p",null,"We normally use defaults in Logstash settings files, but for Netsage, which uses the Logstash Aggregation filter, it is ",Object(i.b)("strong",{parentName:"p"},"required to use only ONE logstash pipeline worker"),"."),Object(i.b)("p",null,"IMPORTANT:  Be sure to set ",Object(i.b)("inlineCode",{parentName:"p"},"pipeline.workers: 1")," in /etc/logstash/logstash.yml and/or /etc/logstash/pipelines.yml. When running logstash on the command line, use ",Object(i.b)("inlineCode",{parentName:"p"},"-w 1"),"."),Object(i.b)("p",null,'The Logstash config files containing the "filters" that comprise the Pipeline are installed in /etc/logstash/conf.d/. Most should be used as-is, but the input (01-) and output (99-) configs may be modified for your use.  The aggregation filter (40-) also has settings that may be changed - check the two timeouts and the aggregation maps path. '),Object(i.b)("blockquote",null,Object(i.b)("p",{parentName:"blockquote"},Object(i.b)("strong",{parentName:"p"},"When processing flows from multiple customers")),Object(i.b)("ul",{parentName:"blockquote"},Object(i.b)("li",{parentName:"ul"},'We use one logstash instance with multiple "logstash-pipelines". The logstash-pipelines are defined in /etc/logstash/pipelines.yml. '),Object(i.b)("li",{parentName:"ul"},"Each logstash-pipeline uses config files in a different directory under /etc/logstash/pipelines/. "),Object(i.b)("li",{parentName:"ul"},"Since most of the config files are the same for all logstash-pipelines, we use symlinks back to files in /etc/logstash/conf.d/."),Object(i.b)("li",{parentName:"ul"},"The exceptions are the input, output, and aggregation files (01-, 99-, and 40-). These are customized so that each logstash-pipeline reads from a different rabbit queue, saves in-progress aggregations to a different file when logstash stops, and writes to a different rabbit queue after processing."),Object(i.b)("li",{parentName:"ul"},"We normally use one input rabbit queue and logstash-pipeline per customer (where one customer may have multiple sensors), but if there are too many sensors, with too much data, we may split them up into 2 or 3 different input queues and pipelines."),Object(i.b)("li",{parentName:"ul"},"The output rabbit queues for processed flows may be on a different host (for us, they are). There, additional independent logstash pipelines can grab the flows and stick them into elasticsearch. Various queues may connect to various ES indices. It's most convenient to put all flows from sensors that will show up in the same granfana portal together in one index (or set of dated indices). "))),Object(i.b)("p",null,'Check the 15-sensor-specific-changes.conf file. When running without Docker, especially with multiple customers, it\'s much easier to replace the contents of that file, which reference environment file values, with hard-coded "if" stagements and clauses that do just what you need.'),Object(i.b)("p",null,"ENV FILE: Our standard processing for Netsage uses the default values for environment variables. These are set directly in the logstash configs. If any of these need to be changed, you can use an environment file: ",Object(i.b)("inlineCode",{parentName:"p"},"/etc/logstash/logstash-env-vars"),". The systemd unit file for logstash is set to read this file if it exists.  You could copy into any or all the logstash-related settings from the env.example file. "),Object(i.b)("p",null,"Note that this file will be read and used by all logstash-pipelines."),Object(i.b)("h2",{id:"pmacct-configuration-and-unit-files"},"Pmacct Configuration and Unit Files"),Object(i.b)("p",null,"Each sensor is assumed to send to a different port on the pipeline host, and each port must have a different collector listening for incoming flow data. With pmacct, these collectors are nfacctd and sfacctd processes. Each requires its own config files and systemd unit file. "),Object(i.b)("p",null,"The easiest way to make the config files is to use the .env file and the setup-pmacct-compose.sh script that were primarily written for use with docker installations.  See the Docker Installation documentation for details.",Object(i.b)("br",{parentName:"p"}),"\n","Doing just a few sensors at a time, edit the .env file and run the script. After running the script, you will find files like nfacctd_1.conf and nfacctd_1-pretag.map in conf-pmacct/ (in the git checkout). "),Object(i.b)("p",null,"You will have to then make the following changes:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Rename the newly created .conf and .map files, replacing _1 with _sensorName (some string that makes sense to humans).  Similarly of _2, etc."),Object(i.b)("li",{parentName:"ul"},"Edit each .conf file and change the name of the .map file within to match (the pre_tag_map value) "),Object(i.b)("li",{parentName:"ul"},"Also, in each .conf file",Object(i.b)("ul",{parentName:"li"},Object(i.b)("li",{parentName:"ul"},"change the port number (nfacctd_port or sfacctd_port) to be the port to which the sensor is sending"),Object(i.b)("li",{parentName:"ul"},'change the rabbit host (amqp_host) from "rabbit" to "localhost"'),Object(i.b)("li",{parentName:"ul"},"change the name of the output rabbit queue (amqp_routing_key) to something unique (eg, netsage_deidentifier_raw_sensorName)"))),Object(i.b)("li",{parentName:"ul"},"Finally, copy the files to /etc/pmacct/\n(You can have the script make some of these changes for you if you temporarily edit the conf-pmacct/*.ORIG files.) ")),Object(i.b)("p",null,"You will also need to create systemd unit files to start and stop each process. Use systemd/sfacctd.service and nfacctd.service as examples. Each should be given a name like nfacctd-sensorName.service. Within the files, edit the config filename in two places. "),Object(i.b)("h2",{id:"start-logstash"},"Start Logstash"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-sh"},"# systemctl start logstash.service\n")),Object(i.b)("p",null,"It will take a minute or two to start. Log files are normally /var/log/messages and /var/log/logstash/logstash-plain.log. ",Object(i.b)("inlineCode",{parentName:"p"},"sudo systemctl status logstash")," is also handy.  "),Object(i.b)("p",null,"Be sure to check to see if it starts ok. If not, look for an error message. If all is ok, the last couple lines should be how many pipelines are running and something about connecting to rabbit."),Object(i.b)("p",null,'NOTE: When logstash is stopped, any flows currently "in the aggregator" will be written out to /tmp/logstash-aggregation-maps (or the path/file set in 40-aggregation.conf). These will be read in and deleted when logstash is started again.  (In some situations, it is desirable to just delete those files before restarting.)'),Object(i.b)("h2",{id:"start-pmacct-processes"},"Start Pmacct Processes"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-sh"},"# systemctl start nfacctd-sensor1\n# systemctl start sfacctd-sensor2\netc.\n")),Object(i.b)("p",null,"After starting these processes, it's good to check the rabbit UI to watch for incoming flow data. Netflow data usually comes in every minute, depending on router settings, and sflow data should come in every 5 minutes since we have set sfacctd to do some pre-aggregation and send results every 5 minutes. You should also see that the messages are consumed by logstash and there is no long-term accumulation of messages in the queue."),Object(i.b)("p",null,"We have noted that in some cases, pmacct is providing so many flows that logstash cannot keep up and the number of messages in the queue just keeps increaseing! This is an issue that has yet to be resolved."),Object(i.b)("p",null,"Flows should exit the pipeline (and appear in Elasticsearch) after about 15 minutes. The delay is due to aggregation. Long-lasting flows will take longer to exit."),Object(i.b)("h2",{id:"cron-jobs"},"Cron jobs"),Object(i.b)("p",null,"Inactive cron files are installed (and provided in the cron.d/ directory of the git checkout). Baremetal-netsage-downloads.cron and restart-logstash-service.cron should be in /etc/cron.d/. Please review and uncomment their contents. "),Object(i.b)("p",null,"These periodically download MaxMind, CAIDA, and Science Registry files, and also restart logstash. Logstash needs to be restarted in order for any updated files to be used."))}u.isMDXComponent=!0},208:function(e,t,n){"use strict";n.d(t,"a",(function(){return p})),n.d(t,"b",(function(){return b}));var a=n(0),s=n.n(a);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var c=s.a.createContext({}),u=function(e){var t=s.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=u(e.components);return s.a.createElement(c.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return s.a.createElement(s.a.Fragment,{},t)}},d=s.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,o=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),p=u(n),d=a,b=p["".concat(o,".").concat(d)]||p[d]||h[d]||i;return n?s.a.createElement(b,l(l({ref:t},c),{},{components:n})):s.a.createElement(b,l({ref:t},c))}));function b(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var l={};for(var r in t)hasOwnProperty.call(t,r)&&(l[r]=t[r]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=n[c];return s.a.createElement.apply(null,o)}return s.a.createElement.apply(null,n)}d.displayName="MDXCreateElement"}}]);